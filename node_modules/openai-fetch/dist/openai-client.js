import { createApiInstance } from './fetch-api.js';
import { StreamCompletionChunker } from './streaming.js';
export class OpenAIClient {
    api;
    constructor(opts = {}) {
        const process = globalThis.process || { env: {} };
        const apiKey = opts.apiKey || process.env['OPENAI_API_KEY'];
        const organizationId = opts.organizationId || process.env['OPENAI_ORG_ID'];
        if (!apiKey)
            throw new Error('Missing OpenAI API key. Please provide one in the config or set the OPENAI_API_KEY environment variable.');
        this.api = createApiInstance({
            apiKey,
            baseUrl: opts.baseUrl,
            organizationId,
            kyOptions: opts.kyOptions,
        });
    }
    getApi(opts) {
        return opts ? this.api.extend(opts) : this.api;
    }
    /** Create a completion for a chat message. */
    async createChatCompletion(params, opts) {
        const response = await this.getApi(opts)
            .post('chat/completions', { json: params })
            .json();
        return response;
    }
    /** Create a chat completion and stream back partial progress. */
    async streamChatCompletion(params, opts) {
        const response = await this.getApi(opts).post('chat/completions', {
            json: { ...params, stream: true },
            onDownloadProgress: () => { }, // trick ky to return ReadableStream.
        });
        const stream = response.body;
        return stream.pipeThrough(new StreamCompletionChunker((response) => response));
    }
    /** Create completions for an array of prompt strings. */
    async createCompletions(params, opts) {
        const response = await this.getApi(opts)
            .post('completions', { json: params })
            .json();
        return response;
    }
    /** Create a completion for a single prompt string and stream back partial progress. */
    async streamCompletion(params, opts) {
        const response = await this.getApi(opts).post('completions', {
            json: { ...params, stream: true },
            onDownloadProgress: () => { }, // trick ky to return ReadableStream.
        });
        const stream = response.body;
        return stream.pipeThrough(new StreamCompletionChunker((response) => response));
    }
    /** Create an embedding vector representing the input text. */
    async createEmbeddings(params, opts) {
        const response = await this.getApi(opts)
            .post('embeddings', { json: params })
            .json();
        return response;
    }
}
//# sourceMappingURL=openai-client.js.map